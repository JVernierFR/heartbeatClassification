import streamlit as st
st.set_page_config(layout="wide")    
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit_utils as su
from plotly import graph_objs as go
from plotly.subplots import make_subplots

import torch.nn as nn
import torch
import joblib
import torch.nn as nn
from os import path, mkdir
import wfdb
import matplotlib
class biggru(nn.Module):
    
    def __init__(self,input_size,hidden_size,num_layer,num_classes=10,dropout=0.4):
        super(biggru,self).__init__()
        self.hidden_size=hidden_size
        self.num_layer=num_layer
        self.block1 =  nn.Sequential(
                        nn.Conv1d(in_channels=1, out_channels=32, kernel_size=7, stride=1, padding=0), # shape: (batch_size, n, 181)
                        nn.ReLU(),
                        nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0), # shape: (batch_size, n, 177)
                        nn.ReLU(),
                        nn.BatchNorm1d(num_features=64),
                        nn.MaxPool1d(3, stride=3)) # shape: (batch_size, n, 59)
        self.gru=nn.GRU(64 ,hidden_size,num_layer,batch_first=True,bidirectional=True)
        self.dropout = nn.Dropout(dropout)
        self.fc1=nn.Linear(hidden_size*59*2,48)
        self.fc2=nn.Linear(48,5)


    def forward(self,x):
        h0=torch.zeros(2*self.num_layer,x.size(0),self.hidden_size).to("cpu")
        out=self.block1(x)
        out,_=self.gru(out.swapaxes(1,2),h0)
        out=out.reshape(out.shape[0],-1)
        out = self.dropout(out)
        out=nn.ReLU()(self.fc1(out))
        out=self.fc2(out)
        
        return out




# variables init:
fs = 125
dict_target1 = {0: 'N',
            1:'S',
            2:'V',
            3:'F',
            4: 'Q'}


# utility to choose a model
# returns:
#   - model
#   - classifier: str model name
#   - func: the function to call to make prediction func(X,model)
#   - dict_target1: dict to transform int classification to str classification
def model_choice():
    st.write("#### Model Choice")
    choix = ['CNN','CNN_2datasets', 'GRU','Binary','XGBoost']
    classifier = st.selectbox('Choose a model', choix)
    
    if classifier == 'CNN':
        model = joblib.load('./models/C2.pkl') 
        func = su.predict_cnn  
    elif classifier == 'CNN_2datasets':
        model = joblib.load('./models/merged_smart_old.pkl')
        func = su.predict_cnn 
    elif classifier == 'GRU':
        model = joblib.load('./models/sequence_conv2grubidir.pkl')
        func = su.predict_cnn 
    elif classifier == 'Binary':
        model = joblib.load('./models/Binary_cnn.pkl')
        func = su.predict_cnn 
    elif classifier == 'XGBoost':
        model = joblib.load('./models/xgboost_model.pkl')
        func = su.predict_xgboost
    if classifier != 'Binary':
        dict_target1 = {0: 'N',
                    1:'S',
                    2:'V',
                    3:'F',
                    4: 'Q'} 
    else:
        dict_target1={0: 'N',1:'Abnormal'}
    return model, classifier, func, dict_target1

fs = 125 
WINDOW = 1500

symbols_to_classif = {'+':'N','.':'N','L':'N','R':'N','e':'N','j':'N',
                      'A':'S','a':'S','J':'S',
                      'E':'V',
                      '/':'Q', 'f':'Q'}


#import pandas as pd
st.title("Hearbeat Classification")

#st.write("Introduction")
#if st.checkbox("Afficher"):
#    st.write("Suite du Streamlit")
st.sidebar.title("Sommaire")
pages=["Introduction", "Dataset","Data Vizualization","Developement phases","Results", "Modelisation","Load Data"]
page=st.sidebar.radio("Aller vers", pages)
from st_kaggle_connector import KaggleDatasetConnection
conn = st.connection("kaggle_datasets", type=KaggleDatasetConnection)
df1 = conn.get(path='shayanfazeli/heartbeat', filename='mitbih_test.csv', ttl=3600)
df1.columns = pd.RangeIndex(df1.shape[1])
target = df1.pop(187)
if page == pages[0] : 
    st.write("### Introduction")
    st.image('./src/streamlit/img/ecg_image.jpeg',width=400)
    st.write('This project proposes an application that allows to check continuous electrocardiograms data from the physioNet database to check for arrhythmia')
    st.write('An electrocardiogram (ECG) is a straightforward test used to assess your heart\'s rhythm and electrical activity. Electrodes placed on your skin detect the electrical signals generated by your heart with each beat.')
    st.write('These signals are recorded by a machine and reviewed by a doctor to identify any abnormalities.')
if page == pages[1] : 
    st.write("### Dataset")
    st.write('This dataset is composed of two collections of heartbeat signals derived from two famous datasets in heartbeat classification, the MIT-BIH Arrhythmia Dataset and The PTB Diagnostic ECG Database.')
    st.write('One dataset contains signals with 5 labels while the cecond one contains a binary classification of normal and abnormal signals')
    st.image('./src/streamlit/img/datasets.png',width=1000)
    st.markdown(
            """
            The dataset consists of :
            - pre-cut signals : 187 points sampled at 125 Hz
            - pre-processed signals normalized between 0 and 1
            """
        )
    st.write('The project focused on the mutli-classes dataset')
    st.write("#### Labels signification")
    col1, col2 = st.columns(2)

    with col1:
        st.write('N')
        st.markdown(
            """
            - Normal
            - Left/Right bundle branch block
            - Atrial escape
            - Nodal escape
            """
        )
        st.write('S')
        st.markdown(
            """
            - Atrial premature
            - Aberrant atrial premature
            - Nodal premature
            - Supra-ventricular premature
            """
        )
       
        st.write('V')
        st.markdown(
            """
            - Premature ventricular contraction
            - Ventricular escape
            """
        )
        st.write('F')
        st.markdown(' -  Fusion of ventricular and normal')
        st.write('Q')
        st.markdown(
            """
            - Paced
            - Fusion of paced and normal
            - Unclassifiable
            """
        )
       
    with col2:
        st.image('./src/streamlit/img/heart.png')



if page == pages[2] : 
    @st.cache_data()
    def showfig(df1,index_of_class,dict_target1):
        fig = plt.figure(figsize=(16,10))
        for cl in range(5):    
            ax = fig.add_subplot(511+cl)
            display_N_signals( np.array(df1),5,index_of_class[cl],ax=ax)
            ax.set_title('Class ' + dict_target1[cl])
        ax.set_xlabel('Time [s]')
        fig.tight_layout(pad=0.5)
        return fig
    st.write("### DataVizualization")
    st.write("a normal ECG shows several peaks due to the contraction of the different part of the heart muscles")
    st.write("A priori, arrhythmia is detected from the amplitude, duration and **delay** between these peaks")

    st.image('./reports/figures/ekg-ecg-interpretation-p-qrs-st-t-wave.jpg',width=600)

    index_of_class = {}
    st.write('Here some vizualisation of several signals',on_change=showfig.clear()   )
    st.button('display random signals again')
    for k in range(5):
        index_of_class[k] =  [i for i, c in enumerate(target) if c==k]
    
    if 'sel_augment' not in st.session_state or st.session_state.sel_augment is None:
        st.session_state.sel_augment = {}
        for clname,cl in zip( ["N","S", "V","F", "Q"],[0,1,2,3,4]):
            jj = np.random.randint(0, high=len(index_of_class[cl]), size=1)[0]
            st.session_state.sel_augment[clname] = index_of_class[cl][jj]
    
    def display_N_signals(X,N,list_of_index,ax=None):
        ns = 187
        if ax is None:
            fig = plt.figure()
            ax = fig.add_subplot()
        list_ii = np.random.choice(list_of_index,N)
        for kii,ii in enumerate(list_ii):
            ax.plot(np.arange(187)/fs,X[ii,:]-np.mean(X[ii,:])+kii*0.75,color='k')
   
    fig = showfig(df1,index_of_class,dict_target1)
    st.pyplot(fig)

    
    st.write("### Data Augmentation")
    st.write("In order to increase the dataset size, each signal was perturbed with random modifications that you can test below")
    st.write("The perturbations remain small.")
    cl_list = ["N","S", "V","F", "Q"]
    cl = st.radio(
    "Choose a class",cl_list
    )
   
    
    selected = st.session_state.sel_augment[cl]  
    power = st.slider("signal power", min_value=0.85, max_value=1.15, value=1.1,step=.05)
    stretch = st.slider("strech", min_value=-15, max_value=15, value=0,step=5)
    linear = st.slider("linear trend", min_value=-0.2, max_value=0.2, value=-0.04,step=0.02)
  

    fig = su.view_signal_augmentation(df1.loc[selected,:].values,power,stretch,linear)
    st.pyplot(fig)
if page == pages[3]:
    with st.container(height=600):
        st.write("### Phase 1: Data Exploration")
        st.write("First phase consisted in expolring the dataset:")
        st.markdown("""
                    - buliding librairies to load and balance the dataset
                    - analyzing signals
                    - computing features applying the pyCatch22 module and peak detections
        """)
        st.write("this first step highlight that the beginning of the signal can be an decisive feature")
        cols= st.columns(2)
        with cols[0]:
            st.image('./src/streamlit/img/Pente.png',width=600,caption='slope of the first 6 samples')
        with cols[1]:
            st.image('./src/streamlit/img/median.png',width=600,caption='median value of the first 6 samples')
        st.write("Additionnaly, attributes of the main 3 peaks were computed (position, height, width)")
        cols= st.columns(2)
        with cols[0]:
            st.image('./src/streamlit/img/peaks_normal.png',width=600)
        with cols[1]:
            st.image('./src/streamlit/img/peaks_abnormal.png',width=600)
        st.write("another dataset can be created from 37 signal features (24 standard pycatch22 and 13 additionals: beginning of the signal, duration, peak features)")
    with st.container(height=600):  
        st.write("### Phase 2: Machine learning")
        st.write("#### 1. SVM")
        st.write("a first SVM model was applied on the features dataset, applying gridsearch on hyperparameter using a undersampled database.")
        st.write("model was applied on oversampled dataset base.")
        st.write("it highlighted that :")
        st.markdown("""
                    - increasing the number of signals has a significant impact on model quality
                    - classes 1 and 3 ar the most difficult classes to predict
                    """)
        st.image('./src/streamlit/img/svm_resutls.png',width=400, caption="calssification metrics on a 13000-siganl-per-class dataset")
        st.write("#### 2. XGboost")
        st.write("Xgboost model was applied on a oversampled database consisting in the raw signal.")
    with st.container(height=600):
        st.write("### Phase 3: Deep learning")
        st.write("#### 1. ANN")
        st.write("Neural network were applied on:")
        st.markdown("""
                - the feature dataset
                - the raw signals
                - with differents resampling (number of items per class)
                - testing activation functions (ReLu, LeakyReLu, Sigmoid)
                - testing number of layers and number of neurones
                """)
        st.write("this modelling highlighted the same issues than with SVM modelling :")
        st.markdown("""
                    - increasing the number of signals has a significant impact on model quality
                    - classes 1 and 3 ar the most difficult classes to predict
                    """)
        st.write("ANN model trained on raw signals (50000 per class) with ReLu activations gives confident results")
        cols= st.columns(2)
        with cols[0]:
            st.image('./src/streamlit/img/DNN_model.png',width=600)        
        with cols[1]:
            st.image('./src/streamlit/img/DNN_results.png',width=600)    
        st.write("#### 2. CNN")   
        st.write("Convolutional neural network were applied on the raw signal:") 
        st.markdown("""
                    - apply wery simple CNN
                    """)
    with st.container(height=600):
        st.write("### Phase 4: improving models")
        st.write("This phase consisted in:")
        st.markdown("""
            - treating overfitting by testing dropout for neural networks and CNN
            - adding batchnormalizaing
            - testing hypermarameters (kernel sizes...)
            - testing adding RNN layer and bi-directionnal GRU layer before the classifier
            - appling interpretability techniques : GradCAM, Occlusion, Shapley Values
            """)
        st.write("Applying Occlusion technique on a signal is the easiest to understand")   
        st.write(":x: adding data augmentation on the fly did not improve the results as expected (adding light gaussian filter for random number of signals for example)")   
        st.write(":white_check_mark: Dropout and batchnormalization improve the results. Finally, architecture is very similar to LeNet or AlexeNet") 
        st.image('./src/streamlit/img/CNN_model.png',width=800)
        st.write("_Note: this work was done before the courses and masterclasses on deep learning_") 
        st.write("#### treating overfitting")
        st.write("Model train on multi-labels dataset are not capable of predicting the binary dataset : predict class 1 or 2 or 3 or 4 for an abnormal signal")
        st.write("A last CNN was train combining both datasets")
        st.markdown("""
                    - 5 output classes [0,1,2,3,4]
                    - true labels of abnormal signals of binary dataset are labelled 5
                    - during training:
                        - labels 5 predicited as 0 replaced by 1 : loss affected
                        - labels 5 predicited as [1,2,3,4] replaced by prediction: not not affected

        """)
    with st.container(height=600):
        st.write("### Phase 5: Transfer learning")
        
if page == pages[4]:
    st.write("### Model results on the test database")
    st.write("The test database contains 21892 signals (mitbhi_test.csv)")
    st.write("Several models were trained from this dataset:")
    st.markdown(
            """
            - a 1D CNN :  architecture similar to AlexNet with  layers of convolution and a neural network classifier: CNN
            - a 1D CNN trained by taking the 2 avaliable datasets. as the second one is only Normal/Abnormal, the loss is penalized if the prediction is [1,2,3,4] for a normal ECG or [0] for an abnormal ECG
            - a 1D CNN including a bidirectionnal GRU layer before the neural network classifier: GRU
            - a 1D CNN trained by taking the 2 avaliable datasets doing a binary classification: Binary
            """
        )
    st.write("Check the results")
    modelnames=  ['CNN','CNN_2datasets','GRU','Binary']
    n_models = len(modelnames)
    cols= st.columns(len(modelnames))
    checks = np.zeros((n_models,))
    for k in range(n_models):
        with cols[k]:
            checks[k] = st.checkbox(modelnames[k] +'model')
    cols= st.columns(len(modelnames)) 
    for k in range(n_models):
        if checks[k]:
            with cols[k]:
                ct = su.confusion_matrix_results(modelnames[k])
                fig = plt.figure(figsize=(5,5))
                ax = fig.add_subplot()
                sns.heatmap(ct,ax=ax,cmap='Blues',annot=True,fmt='d',cbar=False,linecolor='black')
                ax.set_ylabel('True')
                ax.set_xlabel('Predictions')
                st.pyplot(fig)

if page == pages[5] : 

    # function to move the displayed signal window
    def update_start(xx):
        st.session_state.start_ii += xx
        st.session_state.start_ii = max(0,st.session_state.start_ii)
        st.write(st.session_state.start_ii)
        if st.session_state.start_ii + xx > st.session_state.record.p_signal.shape[0]:
            st.session_state.start_ii = st.session_state.record.p_signal.shape[0]-xx
    # display only a part of the signal from 
    # start_ii to start_ii + 
    if 'start_ii' not in st.session_state:
        st.session_state.start_ii = 0
    # save full record and annotation    
    if 'record' not in st.session_state:
        st.session_state.record = None
    if 'annot' not in st.session_state:
        st.session_state.annot = None
    # save patien number
    if 'patient' not in st.session_state:
        st.session_state.patient = None

    st.write("### Modelization")
    st.write('the continuous ECG are available on the PhysioNet website. Choose one patient among the 47 patients')
    st.write('a portion of the ECG is display and analyzed. Use the button to move to the next portion')
    
    st.markdown(
            """
            tips: 
            - class V :  many samples in 106, 119 , 200 , 203, 208, 223
            - class S :  many samples in 232
            - class Q :  many samples in 104, 217 
            """
        )

    # utility to choose a model
    model,classifier,func_prediction,dict_target1 = model_choice()

    

    st.write("#### load ECG from Physionet")   
    
    dataset = 'MIT'
    # read the list of data available on MIT-BIH 
    datalist = su.patient_list_wfdb()
  
    patient_number = st.selectbox(
        "Choose ECG from patient number",
        datalist,
        index=None if (st.session_state.patient is None or st.session_state.patient not in datalist) else datalist.index(st.session_state.patient),
        placeholder="Select ..."
    )
    
    if patient_number is not None and patient_number != st.session_state.patient :       
        st.session_state.patient = patient_number
        st.session_state.start_ii = 0

        #su.download_patient(option)
        #st.session_state.record, st.session_state.annot = su.download_patient(patient_number)
        st.session_state.record, st.session_state.annot = su.stream_patient(patient_number,dataset)
        # resample 
        with st.spinner("Please wait we are now process patient " + patient_number +" data "):
            st.session_state.record, st.session_state.annot = su.resample_record(st.session_state.record, st.session_state.annot , target_fs=125)
        st.success("Patient " +patient_number+" data  have been resampled !")
    
    if patient_number is not None :
        # extract a window
        signal, t, labels_indexes, labels = su.extract(st.session_state.record,
                                                    st.session_state.annot,st.session_state.start_ii,WINDOW,fs=fs)
        # split to a a batch
        X, y_true, y_index = su.annotated_signal_to_X_and_y(signal,labels_indexes,labels)
       

        if st.session_state.start_ii + WINDOW > st.session_state.record.p_signal.shape[0]:
            st.session_state.start_ii = 0 

        # predict
        y_pred = func_prediction(X,model)

        # convert to string labels
        y_pred_str = [dict_target1[x] for x in y_pred]

        labels = pd.Series(labels).replace(symbols_to_classif).values
        #visualize
        fig = su.signal_predictions_true(t,signal,labels_indexes,labels,y_index,y_pred_str)
        st.pyplot(fig)

        # next window
        col1, col2,*_ = st.columns(8)

        with col2:
            st.button('Next', on_click=update_start,args=[WINDOW])

        with col1:
            st.button('Previous', on_click=update_start,args=[-WINDOW])
# load .dat and .hea       
if page == pages[6]:
    if 'loaded_dat' not in st.session_state:
        st.session_state.loaded_dat = None
    if 'loaded_hea' not in st.session_state:
        st.session_state.loaded_hea = None  
    if 'update_fig_only' not in st.session_state:
        st.session_state.update_fig_only = False
    st.write('load a .dat file and a .hea file corresponding to ECG data and headers. ')        
    st.write('Data can be found on physio net for example here: https://physionet.org/content/mimic-iv-ecg/1.0/files/#files-panel or https://physionet.org/content/mimic-iv-ecg/1.0/files/#files-panel ...')  
   
    if not path.isdir('tempDir'):
        mkdir('tempDir')
    st.session_state.loaded_dat = st.file_uploader("Choose a file",type='dat') 
    if st.session_state.loaded_dat is not None:
        st.session_state.loaded_hea = st.file_uploader("Choose a file",type='hea')  
    else:
        st.session_state.loaded_hea = None

    if st.session_state.loaded_dat is not None and st.session_state.loaded_hea is not None and\
       st.session_state.loaded_dat.name.replace('.dat','') == st.session_state.loaded_hea.name.replace('.hea',''):
                with open(path.join("tempDir",st.session_state.loaded_dat.name),"wb") as f:
                    f.write(st.session_state.loaded_dat.getbuffer())
                with open(path.join("tempDir",st.session_state.loaded_hea.name),"wb") as f:
                    f.write(st.session_state.loaded_hea.getbuffer())
                file = path.join("tempDir",st.session_state.loaded_dat.name.replace('.dat',''))
                record = wfdb.io.rdrecord(file)
                st.write('available leads')
                st.write(record.sig_name)
                
    else:
        record = None
        
    model,classifier,func,dict_target1 = model_choice()

    if record is not None:
        
        isOK,msg, y_pred, qrs, ecg = su.process_new_record(record,func,model,target_fs = 125)
        st.write(msg)
        if isOK:
            
            y_pred_str = [dict_target1[x] for x in y_pred]

            with st.spinner("Please wait ... "):
                my_layout = go.Layout({
                        "yaxis": {"title":""},
                        "xaxis": {"title":"Time [s]"},
                        "showlegend": False})
                fig = go.Figure(layout=my_layout )

                make_subplots(rows=2, cols=1,shared_xaxes=True,row_heights=[1,0.1],figure=fig)
                t = np.arange(ecg.shape[0])/fs
                ylim = [min(ecg),max(ecg)]
                fig.add_trace(go.Scatter(y = ecg,x=t),row=1,col=1)
                for tx in t[qrs]:
                    fig.add_trace(go.Scatter(x=[tx,tx],y=ylim, mode='lines',line=dict(color='black',  dash='dot')),row=1,col=1)
                    fig.add_trace(go.Scatter(x=[tx,tx],y=ylim, mode='lines',line=dict(color='black',  dash='dot')),row=2,col=1)
                for tx,ys in zip(t[qrs],y_pred_str):
                        c = 'black' if ys == 'N' else 'red'
                        fig.add_trace(go.Scatter( x=[float(tx)], y=[1], mode="text",text='   '+ys,textfont_color=c),row=2,col=1)
                fig.update_yaxes(range=[0, 2],showticklabels=False, row=2, col=1)    
                st.plotly_chart(fig)
            